# -*- coding: utf-8 -*-
"""Veri_İşleme

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13Dvh4nS00D4Yttevy14INwl-dcTKzpza
"""

#Importlar
#Pip ve importlar orijinal olarak Google Colab'de yapıldığı için Colab dışında çalışmayabilirler

! wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.8.2-Linux-x86_64.sh
! chmod +x Miniconda3-py37_4.8.2-Linux-x86_64.sh
! bash ./Miniconda3-py37_4.8.2-Linux-x86_64.sh -b -f -p /usr/local
! conda install -c rdkit rdkit -y
import sys
sys.path.append('/usr/local/lib/python3.7/site-packages/')

import networkx as nx
from rdkit import Chem
from rdkit.Chem import Descriptors, AllChem
from rdkit.Chem.Descriptors import qed, MolLogP
from rdkit.Chem import RDConfig
import os
import sys
sys.path.append(os.path.join(RDConfig.RDContribDir, 'SA_Score'))
import sascorer

!pip install selfies
!pip install SmilesPE

from selfies import encoder
from SmilesPE.pretokenizer import atomwise_tokenizer

import numpy as np
import pandas as pd
import os
import tensorflow as tf
import tensorflow_probability as tfp
from tensorflow import clip_by_global_norm
from tensorflow.keras.callbacks import TensorBoard
from tensorflow.keras.initializers import lecun_normal
from tensorflow.keras.optimizers import Adam,SGD
from tensorflow.keras.losses import binary_crossentropy,MAE
from tensorflow.keras.layers import MaxPool1D,Attention,Conv1DTranspose,GRU,LSTM,Conv1D,Bidirectional,Dense,Input,RepeatVector,Reshape,Flatten,AveragePooling1D,GlobalAvgPool1D,GlobalMaxPool1D,Lambda,AlphaDropout,TimeDistributed,Embedding,Activation,BatchNormalization,LayerNormalization
from tensorflow import keras
from tensorflow.keras import Model
from tensorflow.keras.layers import Concatenate as concat
import tensorflow.keras.backend as back
from tensorflow.keras.metrics import BinaryAccuracy
from tensorflow.math import reduce_logsumexp, reduce_mean, reduce_sum
from tensorflow.keras.backend import clip
from tensorflow.linalg import tensor_diag
from tensorflow.nn import softplus
from collections import Counter
import sklearn
from sklearn.mixture import BayesianGaussianMixture, GaussianMixture
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from time import time

from google.colab import drive
drive.mount('/content/drive')

#MAE Veri Seti

moses = pd.read_csv("https://media.githubusercontent.com/media/molecularsets/moses/master/data/train.csv")

moses = moses["SMILES"]
moses.drop_duplicates(inplace = True)

#CSV olarak ChEMBL ana veri seti alınır
a = pd.read_csv()[["Smiles","QED Weighted"]]
#CSV olarak ChEMBL Covid-19 veri seti alınır
b = pd.read_csv()[["Smiles","QED Weighted"]]
chembl = a.copy()
chembl.drop_duplicates(inplace = True)
b.drop_duplicates(inplace = True)

smiles_chembl = chembl["Smiles"]
smiles_moses = moses.copy()
smiles_chembl.drop_duplicates(inplace = True)
smiles_moses.drop_duplicates(inplace = True)
smiles_chembl = smiles_chembl.to_numpy()
smiles_moses = smiles_moses.to_numpy()
corona_smiles = b["Smiles"].to_numpy()

#Chembl
qed_values_chembl = np.empty(shape = (len(smiles_chembl,)))
for i in range(len(smiles_chembl)):
  qed_values_chembl[i] = qed(Chem.MolFromSmiles(smiles_chembl[i]))

#Moses
qed_values_moses = np.empty(shape = (len(smiles_moses,)))
for i in range(len(smiles_moses)):
  qed_values_moses[i] = qed(Chem.MolFromSmiles(smiles_moses[i]))

smiles_chembl_copy = smiles_chembl.copy()
faulty_index = []
for i in range(len(smiles_chembl)):
  if qed_values_chembl[i] < 0.5:
    faulty_index.append(i)
smiles_chembl_copy = np.delete(smiles_chembl_copy,faulty_index)

smiles_moses_copy = smiles_moses.copy()
faulty_index = []
for i in range(len(smiles_moses)):
  if qed_values_moses[i] < 0.9:
    faulty_index.append(i)
smiles_moses_copy = np.delete(smiles_moses_copy,faulty_index)

smiles = np.append(smiles_chembl_copy,smiles_moses_copy)

smiles_df = pd.DataFrame(smiles)
smiles_df.drop_duplicates(inplace = True)
smiles = smiles_df.to_numpy()

smiles = smiles.reshape((len(smiles),))

corona_selfies = []
for i in range(len(corona_smiles)):
  try:
    temp = encoder(corona_smiles[i])
    corona_selfies.append(atomwise_tokenizer(temp))
  except:
    print(i)

selfies = []
for i in range(len(smiles)):
  try:
    temp = encoder(smiles[i])
    selfies.append(atomwise_tokenizer(temp))
  except:
    print(i)

corona_max = 0
for i in corona_selfies:
  if len(i) > corona_max:
    corona_max = len(i)

max = 0
for i in selfies:
  if len(i) > max:
    max = len(i)

corona_lengths = []
for i in corona_selfies:
  corona_lengths.append(len(i))
corona_lengths = pd.DataFrame(corona_lengths)

corona_distribution = corona_lengths[0].plot.hist(bins = 12,alpha = 0.5)

corona_lengths = {}
for i in corona_selfies:
  if len(i) not in corona_lengths:
    corona_lengths[len(i)] = 1
  else:
    corona_lengths[len(i)] += 1
  if len(i) == 1:
    print(corona_selfies.index(i))

lengths = []
for i in selfies:
  lengths.append(len(i))
lengths = pd.DataFrame(lengths)

distribution = lengths[0].plot.hist(bins = 12,alpha = 0.5)

lengths = {}
for i in selfies:
  if len(i) not in lengths:
    lengths[len(i)] = 1
  else:
    lengths[len(i)] += 1
  if len(i) == 1:
    print(selfies.index(i))

chosen_corona_max = 128
corona_selfies_copy_1 = corona_selfies.copy()
corona_selfies_copy_2 = corona_selfies.copy()

for i in corona_selfies_copy_1:
  if len(i) > chosen_corona_max:
    corona_selfies_copy_2.remove(i)

corona_selfies_copy = corona_selfies_copy_2

num = 0
for i in corona_selfies_copy:
  if len(i) > 182:
    num += 1
print(num)

chosen_max = 128
selfies_copy = selfies.copy()
selfies_copy_1 = selfies.copy()

indexes = []
for i in range(len(selfies_copy_1)):
  if len(selfies_copy_1[i]) > chosen_max:
    indexes.append(i)
    selfies_copy.remove(selfies_copy_1[i])

down_limit = 10
for i in corona_selfies_copy:
  if len(i) < down_limit:
    corona_selfies_copy.remove(i)

num = 0
for i in corona_selfies_copy:
  if len(i) < down_limit:
    num+=1
print(num)

down_limit = 10
for i in selfies_copy:
  if len(i) < down_limit:
    selfies_copy.remove(i)

num = 0
for i in selfies_copy:
  if len(i) < down_limit:
    num+=1
print(num)

labels = ['[C]',
 '[#C]',
 '[=C]',
 '[c]',
 '[=c]',
 '[-c]',
 '[N]',
 '[#N]',
 '[=N]',
 '[-n]',
 '[n]',
 '[N+expl]',
 '[=N+expl]',
 '[=N-expl]',
 '[NHexpl]',
 '[=NH2+expl]',
 '[O]',
 '[=O]',
 '[o]',
 '[Oexpl]',
 '[O-expl]',
 '[F]',
 '[P]',
 '[S]',
 '[=S]',
 '[s]',
 '[Cl]',
 '[Cl-expl]',
 '[Br]',
 '[Br-expl]',
 '[I]',
 '[I-expl]',
 '[nHexpl]',
 '[Hexpl]',
 '[epsilon]',
 '[Branch1_1]',
 '[Branch1_2]',
 '[Branch1_3]',
 '[Branch2_1]',
 '[Branch2_2]',
 '[Branch2_3]',
 '[Branch3_3]',
 '[Branch3_1]',
 '[Branch3_2]',
 '[Ring1]',
 '[Expl-Ring1]',
 '[Expl=Ring1]',
 '[Ring2]',
 '[Expl-Ring2]',
 '[Expl=Ring2]',
 '[Ring3]',
 '.',
 '[C@@Hexpl]',
 '[C@Hexpl]',
 '[C@expl]',
 '[C@@expl]',
 '[/C]',
 '[\\C]',
 '[/S]',
 '[Expl\\Ring1]',
 '[/N]',
 '[\\N]',
 '[/C@Hexpl]',
 '[H+expl]',
 '[S-expl]',
 '[Zn+2expl]',
 '[Siexpl]',
 '[Na+expl]',
 '[\\S]',
 '[/O]',
 '[P@expl]',
 '[Expl/Ring2]',
 '[N-expl]',
 '[B]',
 '[\\C@@Hexpl]',
 '[/C@@Hexpl]',
 '[=S+expl]',
 '[Cexpl]',
 '[=Cexpl]',
 '[/Br]',
 '[/Cexpl]',
 '[/Cl]',
 '[Znexpl]',
 '[P@@expl]']

new_corona_labels = []
for i in corona_selfies_copy:
  for j in i:
    if(j not in labels) and (j not in new_corona_labels):
      new_corona_labels.append(j)

corona_label_num = 0
for i in corona_selfies_copy:
  for j in i:
    try:
      a = new_corona_labels.index(j)
      corona_label_num += 1
      break
    except:
      pass

corona_label_dict = dict(zip(new_corona_labels,[0 for i in range(37)]))
for i in corona_selfies_copy:
  for j in i:
    try:
      corona_label_dict[j] += 1
    except:
      pass

new_labels = []
for i in concated_selfies:
  for j in i:
    if (j not in labels) and (j not in new_labels):
      new_labels.append(j)

declined_labels = new_corona_labels + new_labels

accepted_labels = labels

label_dict = dict(zip(accepted_labels,range(1,len(accepted_labels)+1)))

for a in range(3):
  for i in concated_selfies:
    for j in i:
      try:
        ind = declined_labels.index(j)
        concated_selfies.remove(i)
        break
      except:
        pass

final_selfies = np.zeros(shape = (len(concated_selfies),chosen_max),dtype = object)
for i in range(len(concated_selfies)):
  for j in range(len(concated_selfies[i])):
    final_selfies[i][j] = concated_selfies[i][j]

numbered_selfies = np.zeros(shape = (len(final_selfies),chosen_max),dtype = "float32")
for i in range(len(final_selfies)):
  for j in range(len(final_selfies[i])):
    if final_selfies[i][j] != 0:
      numbered_selfies[i][j] = label_dict[final_selfies[i][j]]
    else:
      break

#MAE Veri Setini Kaydetme
np.save("",numbered_selfies)

#DeepADTP Veri Seti

#Ayrıştırılmış KIBA Veri Seti çekilir
test = pd.read_csv("https://raw.githubusercontent.com/zhaoqichang/AttentionDTA_BIBM/master/tfrecord/kiba_str_all.txt",header = None,sep = " ")

kiba_mols = test[2]
kiba_proteins = test[3]
kiba_bindaff = test[4]

kiba_mols = kiba_mols.to_numpy()
kiba_proteins = kiba_proteins.to_numpy()
kiba_bindaff = kiba_bindaff.to_numpy()

labels = ['[C]',
 '[#C]',
 '[=C]',
 '[c]',
 '[=c]',
 '[-c]',
 '[N]',
 '[#N]',
 '[=N]',
 '[-n]',
 '[n]',
 '[N+expl]',
 '[=N+expl]',
 '[=N-expl]',
 '[NHexpl]',
 '[=NH2+expl]',
 '[O]',
 '[=O]',
 '[o]',
 '[Oexpl]',
 '[O-expl]',
 '[F]',
 '[P]',
 '[S]',
 '[=S]',
 '[s]',
 '[Cl]',
 '[Cl-expl]',
 '[Br]',
 '[Br-expl]',
 '[I]',
 '[I-expl]',
 '[nHexpl]',
 '[Hexpl]',
 '[epsilon]',
 '[Branch1_1]',
 '[Branch1_2]',
 '[Branch1_3]',
 '[Branch2_1]',
 '[Branch2_2]',
 '[Branch2_3]',
 '[Branch3_3]',
 '[Branch3_1]',
 '[Branch3_2]',
 '[Ring1]',
 '[Expl-Ring1]',
 '[Expl=Ring1]',
 '[Ring2]',
 '[Expl-Ring2]',
 '[Expl=Ring2]',
 '[Ring3]',
 '.',
 '[C@@Hexpl]',
 '[C@Hexpl]',
 '[C@expl]',
 '[C@@expl]',
 '[/C]',
 '[\\C]',
 '[/S]',
 '[Expl\\Ring1]',
 '[/N]',
 '[\\N]',
 '[/C@Hexpl]',
 '[H+expl]',
 '[S-expl]',
 '[Zn+2expl]',
 '[Siexpl]',
 '[Na+expl]',
 '[\\S]',
 '[/O]',
 '[P@expl]',
 '[Expl/Ring2]',
 '[N-expl]',
 '[B]',
 '[\\C@@Hexpl]',
 '[/C@@Hexpl]',
 '[=S+expl]',
 '[Cexpl]',
 '[=Cexpl]',
 '[/Br]',
 '[/Cexpl]',
 '[/Cl]',
 '[Znexpl]',
 '[P@@expl]']

label_dict = dict(zip(labels,range(1,len(labels)+1)))

kiba_mols_selfies = []

for i in kiba_mols:
  temp = encoder(i)
  kiba_mols_selfies.append(atomwise_tokenizer(temp))

test_kiba_mols_selfies = kiba_mols_selfies

kiba_mols_tokenized = []
kiba_temp_proteins = []
kiba_temp_bindaff = []
for i in range(len(kiba_mols_selfies)):
  if len(kiba_mols_selfies[i]) <= 128:
    temp = []
    for j in range(128):
      try:
        temp.append(label_dict[kiba_mols_selfies[i][j]])
      except:
        temp.append(0)
    kiba_mols_tokenized.append(temp)
    kiba_temp_proteins.append(kiba_proteins[i])
    kiba_temp_bindaff.append(kiba_bindaff[i])

error_indexes = []
for i in range(len(kiba_mols_tokenized)):
  zero_found = False
  for j in range(len(kiba_mols_tokenized[i])):
    if (kiba_mols_tokenized[i][j] == 0) and (zero_found == False):
      zero_found = True
    elif (kiba_mols_tokenized[i][j] != 0) and (zero_found == True):
      error_indexes.append([i,j])
      break

protein_dict = {'A': 7,
 'C': 11,
 'D': 17,
 'E': 10,
 'F': 3,
 'G': 2,
 'H': 19,
 'I': 20,
 'K': 5,
 'L': 14,
 'M': 6,
 'N': 15,
 'P': 8,
 'Q': 12,
 'R': 4,
 'S': 1,
 'T': 13,
 'V': 9,
 'W': 16,
 'Y': 18}

remove_indexes = []
kiba_proteins_tokenized = []
for i in range(len(kiba_temp_proteins)):
  if len(kiba_temp_proteins[i]) <= 1500:
    temp = []
    for j in kiba_temp_proteins[i]:
      temp.append(protein_dict[j])
    kiba_proteins_tokenized.append(temp)
  else:
    remove_indexes.append(i)

kiba_proteins_tokenized_copy = kiba_proteins_tokenized.copy()
kiba_proteins_tokenized = np.zeros(shape = (len(kiba_proteins_tokenized),1500))
for i in range(len(kiba_proteins_tokenized_copy)):
  for j in range(len(kiba_proteins_tokenized_copy[i])):
    kiba_proteins_tokenized[i][j] = kiba_proteins_tokenized_copy[i][j]

for i in remove_indexes[::-1]:
  kiba_mols_tokenized.pop(i)
  kiba_temp_bindaff.pop(i)

kiba_mols_tokenized_copy = kiba_mols_tokenized.copy()
kiba_mols_tokenized = np.zeros(shape = (len(kiba_mols_tokenized),128))
for i in range(len(kiba_mols_tokenized_copy)):
  for j in range(len(kiba_mols_tokenized_copy[i])):
    kiba_mols_tokenized[i][j] = kiba_mols_tokenized_copy[i][j]

kiba_temp_bindaff = np.asarray(kiba_temp_bindaff)

#Molekül, Protein ve Binding Affinity Verileri kaydedilir
np.save("",kiba_mols_tokenized)
np.save("",kiba_proteins_tokenized)
np.save("",kiba_temp_bindaff)